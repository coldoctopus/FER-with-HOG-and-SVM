{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential files from /src folder\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from hog import preprocessing_split, preprocessing_full\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import show_plot, transform_data\n",
    "\n",
    "# for saving the model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "ker = 'poly'  # kernel type, change to 'linear', 'rbf', 'sigmoid' or 'poly' for different kernels\n",
    "deg = 3\n",
    "gam = 0.1\n",
    "c = 75\n",
    "coef = 1\n",
    "date = '02082025'   #ddmmyyyy: date of the model\n",
    "time = '095138'   #hhmmss: time of the model\n",
    "\n",
    "model = joblib.load(f'../models/kernel-{ker}_deg-{deg}_gamma-{gam}_C-{c}_coef0-{coef}_{date}_{time}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "path = \"D:/GitHub/FER-with-HOG-and-SVM/datasets/train/sample_1\"     # path to dataset, change your path here\n",
    "data_train, label_train, data_test, label_test = preprocessing_split(path)\n",
    "X_train , y_train = transform_data(data_train, label_train)\n",
    "X_test , y_test = transform_data(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = metrics.classification_report(y_test, prediction, target_names=['anger', 'happy', 'neutral', 'sad', 'suprise'],output_dict=True)\n",
    "rp = pd.DataFrame(rp).transpose()\n",
    "rp.drop(\"support\",axis=1,inplace=True)\n",
    "rp.drop(\"macro avg\",axis=0,inplace=True)\n",
    "rp.drop(\"weighted avg\",axis=0,inplace=True)\n",
    "rp.drop(\"accuracy\",axis=0,inplace=True)\n",
    "rp = round(rp.iloc[0:]*100,2)\n",
    "rp\n",
    "\n",
    "# # note on metrics:\n",
    "# # precision: how often the model is correct when it predicts a class (true positives / (true positives + false positives))\n",
    "# high value => when model predicts a class, it is likely to be correct\n",
    "# # recall: how often the model predicts a class when it is actually that class (true positives / (true positives + false negatives))\n",
    "# high value => when the class is present, the model is likely to predict it\n",
    "# # f1: harmonic mean of precision and recall, useful when you want to balance both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(\"Precision\",rp.precision)\n",
    "show_plot(\"Recall\",rp.recall)\n",
    "show_plot(\"F1 Score\", rp['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, prediction)\n",
    "confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "acc_per_class = confusion_matrix.diagonal()\n",
    "apc = pd.DataFrame(index=['anger', 'happy', 'neutral', 'sad', 'suprise'],data=acc_per_class,columns=['Accuracy per class'])\n",
    "apc = round(apc.iloc[0:]*100,2)\n",
    "show_plot(\"Accuracy per class\", apc['Accuracy per class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy of the model\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, prediction)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [True, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "#from 0 to 4: anger, happy, neutral, sad, suprise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
